# -*- coding: utf-8 -*-
"""GST_Predictive_Modelling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rIBVd6Z1GHrLWuS1rKUjsDLjaAe20E8S
"""

!pip install dataprep

import pandas as pd
import numpy as np
from dataprep.eda import plot, plot_correlation, create_report, plot_missing

#Loading Test and Train datasets
X_Train = pd.read_csv("/content/X_Train_Data_Input.csv")
Y_Train = pd.read_csv("/content/Y_Train_Data_Target.csv")
X_Test = pd.read_csv("/content/X_Test_Data_Input.csv")
Y_Test = pd.read_csv("/content/Y_Test_Data_Target.csv")

#Plotting the datasets to check on stats and Insights
plot(X_Test)

#Merging Targets and data for both test and train sets
train_data = pd.merge(X_Train,Y_Train,on='ID')
test_data = pd.merge(X_Test,Y_Test,on='ID')

print(train_data.shape)
print(test_data.shape)

"""Train dataset contains 785133 rows and 24 columns
Test dataset contains 261712 rows and 24 columns
"""

#Removing ID column from both datasets. As unique column has overfitting risk
train_data.drop('ID', axis=1, inplace=True)
test_data.drop('ID', axis=1, inplace=True)

#Checking Train_Data
train_data.describe()

#Checking Test_Data
test_data.describe()

#Checking summary of Train dataset
train_data.info()

#Checking summary of Test dataset
test_data.info()

#Checking the no. of null values in each and every column for both Test and Train Datasets
train_data.isnull().sum()
test_data.isnull().sum()

#Checking the percentage of missing values in both Train and Test Datasets
train_data_null_value_percentage = (train_data.isnull().sum() / len(X_Train))*100
test_data_null_value_percentage = (test_data.isnull().sum() / len(X_Test))*100
print(train_data_null_value_percentage)
print(test_data_null_value_percentage)

#Dropping column9 as it contains 93% of missing values
train_data.drop('Column9', axis=1, inplace=True)
test_data.drop('Column9', axis=1, inplace=True)

train_data.shape

#Using Median Imputation as the data is skewed

#We are not dropping any rows as we are not sure about the description of features
for column in ['Column3', 'Column4', 'Column5','Column6','Column8', 'Column14','Column15']:
    train_data[column].fillna(train_data[column].median(), inplace=True)

#Column0 contains only 9 missing values. So, using mode imputation
# Calculate the mode for Column0
mode_value_column0 = train_data['Column0'].mode()[0]  # Calculate the mode

# Replacing missing values in Column0 with the mode
train_data['Column0'].fillna(mode_value_column0, inplace=True)


for column in ['Column3', 'Column4', 'Column5','Column6','Column8', 'Column14','Column15']:
    test_data[column].fillna(test_data[column].median(), inplace=True)

#Column0 contains only 9 missing values. So, using mode imputation
# Calculate the mode for Column0
mode_value_column0 = test_data['Column0'].mode()[0]  # Calculate the mode

# Replacing missing values in Column0 with the mode
test_data['Column0'].fillna(mode_value_column0, inplace=True)

#Checking for missing values in both Test and Train Datasets
train_data.isnull().sum()
test_data.isnull().sum()

"""All the null values are being handled"""

#Train_Dataset.to_csv('Train_Dataset.csv', index=False)

#Test_Dataset.to_csv('Test_Dataset.csv',index=False)

train_data.shape

"""Null values are handled in the dataset"""

#Class Balancing for Test Dataset
from imblearn.over_sampling import SMOTE

# Separate features (X) and target (y)
X_test = train_data.drop('target', axis=1)
y_test = train_data['target']

# Apply SMOTE to balance the classes
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_test, y_test)

# Combine resampled data back into a DataFrame
train_data_with_class_balance = pd.concat([pd.DataFrame(X_resampled), pd.DataFrame(y_resampled, columns=['target'])], axis=1)

print(train_data_with_class_balance['target'].value_counts())  # To check if the classes are balanced

#Class Balancing for Test Dataset
from imblearn.over_sampling import SMOTE

# Separate features (X) and target (y)
X_test = test_data.drop('target', axis=1)
y_test = test_data['target']

# Apply SMOTE to balance the classes
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_test, y_test)

# Combine resampled data back into a DataFrame
test_data_with_class_balance = pd.concat([pd.DataFrame(X_resampled), pd.DataFrame(y_resampled, columns=['target'])], axis=1)

print(test_data_with_class_balance['target'].value_counts())  # To check if the classes are balanced

# #Removing Outliers for train data

# import numpy as np
# import pandas as pd

# def remove_outliers_iqr(df):
#     Q1 = df.quantile(0.25)
#     Q3 = df.quantile(0.75)
#     IQR = Q3 - Q1
#     lower_bound = Q1 - 1.5 * IQR
#     upper_bound = Q3 + 1.5 * IQR
#     df_out = df[~((df < lower_bound) | (df > upper_bound)).any(axis=1)]
#     return df_out

# train_data_with_class_balance_outliers = remove_outliers_iqr(train_data_with_class_balance)

# #test_data = remove_outliers_iqr(test_data)

# #Removing Outliers for test data

# import numpy as np
# import pandas as pd

# def remove_outliers_iqr(df):
#     Q1 = df.quantile(0.25)
#     Q3 = df.quantile(0.75)
#     IQR = Q3 - Q1
#     lower_bound = Q1 - 1.5 * IQR
#     upper_bound = Q3 + 1.5 * IQR
#     df_out = df[~((df < lower_bound) | (df > upper_bound)).any(axis=1)]
#     return df_out

# test_data_with_class_balance_outliers = remove_outliers_iqr(test_data_with_class_balance)

# #test_data = remove_outliers_iqr(test_data)

print(train_data_with_class_balance.shape)
print(test_data_with_class_balance.shape)

# #Renaming the dataframes
# train_data = train_data_with_class_balance_outliers
# test_data = test_data_with_class_balance_outliers

# print(train_data.shape)
# print(test_data.shape)

#Corelation matrix for Train data
import seaborn as sns
import matplotlib.pyplot as plt

# Compute the correlation matrix
corr_matrix = train_data.corr()

# Plot heatmap for better visualization
plt.figure(figsize=(8,8))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm")
plt.title("Correlation Matrix")
plt.show()

#Corelation matrix for Test data
import seaborn as sns
import matplotlib.pyplot as plt

# Compute the correlation matrix
corr_matrix = test_data.corr()

# Plot heatmap for better visualization
plt.figure(figsize=(8,8))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm")
plt.title("Correlation Matrix")
plt.show()

import pandas as pd
import numpy as np

# Assuming 'train_data' and 'test_data' are your loaded dataframes

# 1. Compute the correlation matrix for train_data
corr_matrix = train_data.corr().abs()

# 2. Select the upper triangle of the correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

# 3. Find features with correlation greater than 0.85 (or any other threshold you prefer)
threshold = 0.85
to_drop = [column for column in upper.columns if any(upper[column] > threshold)]

print(f"Features to remove due to high correlation: {to_drop}")

# 4. Drop these features from both train_data and test_data
X_train_cleaned = train_data.drop(columns=to_drop)
X_test_cleaned = test_data.drop(columns=to_drop)

# Split the cleaned train and test data into features and target
X_train = X_train_cleaned.iloc[:, :-1]  # All columns except the target
y_train = X_train_cleaned.iloc[:, -1]   # The target column
X_test = X_test_cleaned.iloc[:, :-1]    # All columns except the target
y_test = X_test_cleaned.iloc[:, -1]     # The target column

#dropping column 4 due to high correlation
train_data.drop('Column4', axis=1, inplace=True)
test_data.drop('Column4', axis=1, inplace=True)

print(train_data.shape)
print(test_data.shape)

print(train_data.columns)

#Removing duplicate rows
# Remove duplicate rows from train_data
train_data = train_data.drop_duplicates()
test_data= test_data.drop_duplicates()

print(train_data.shape,test_data.shape)

#Feature Scaling for Test_Dataset
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.preprocessing import FunctionTransformer

def log_transform(data):
    transformer = FunctionTransformer(np.log1p, validate=True)
    return transformer.transform(data)

test_data['Column0'] = log_transform(test_data['Column0'].values.reshape(-1, 1))

# Min-Max Scaling for column0 #Heavily skewed data
min_max_scaler = MinMaxScaler()
test_data['Column0'] = min_max_scaler.fit_transform(test_data[['Column0']])

# Standard Scaling for column1, skewness and kurtosis are close to normal.
standard_scaler = StandardScaler()
test_data['Column1'] = standard_scaler.fit_transform(test_data[['Column1']])

# Robust Scaling for column2
robust_scaler = RobustScaler()
test_data['Column2'] = robust_scaler.fit_transform(test_data[['Column2']])

# Display the first few rows of the scaled data
print(test_data.head())



#Feature Importance via RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier

# Assume df_train has your features and 'target' is the target column
X_train = train_data.drop('target', axis=1)
y_train = train_data['target']

X_test = test_data.drop('target', axis=1)
y_test = test_data['target']

# Train a random forest classifier
rf = RandomForestClassifier()
rf.fit(X_train, y_train)

# Get feature importance
importances = rf.feature_importances_
feature_names = X_train.columns

# Create a DataFrame to visualize
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Visualize feature importance
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.show()

# Step 1: Set a threshold for importance (you can adjust this based on the chart)
importance_threshold = 0.01  # Features with importance < 0.01 will be dropped

# Step 2: Filter the features based on the threshold
important_features = feature_importance_df[feature_importance_df['Importance'] > importance_threshold]['Feature']

print(f"Features to keep: {list(important_features)}")

# Step 3: Update train and test sets to include only important features
X_train_important = X_train[important_features]
X_test_important = X_test[important_features]

# Step 4: Retrain the Random Forest with only important features
rf_important = RandomForestClassifier(random_state=42)
rf_important.fit(X_train_important, y_train)

# Step 5: Evaluate the model
y_pred_important = rf_important.predict(X_test_important)

# Import all necessary metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score

# Step 6: Calculate and print all relevant evaluation metrics
accuracy = accuracy_score(y_test, y_pred_important)
precision = precision_score(y_test, y_pred_important, average='weighted')
recall = recall_score(y_test, y_pred_important, average='weighted')
f1 = f1_score(y_test, y_pred_important, average='weighted')

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")

# Step 7: (Optional) Compute ROC-AUC if it's a binary classification
if len(y_test.unique()) == 2:
    y_prob_important = rf_important.predict_proba(X_test_important)[:, 1]
    roc_auc = roc_auc_score(y_test, y_prob_important)
    print(f"ROC-AUC Score: {roc_auc:.2f}")

# Step 8: Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_important)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

from sklearn.metrics import classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Calculate precision, recall, F1-score, and support
print("Classification Report:")
print(classification_report(y_test, y_pred_important))

# ROC-AUC for binary classification
if len(y_test.unique()) == 2:
    y_prob_important = rf_important.predict_proba(X_test_important)[:, 1]  # Probability estimates for the positive class
    roc_auc = roc_auc_score(y_test, y_prob_important)
    print(f"ROC-AUC Score: {roc_auc:.2f}")

    # Plot the ROC curve
    fpr, tpr, thresholds = roc_curve(y_test, y_prob_important)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f'ROC Curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], linestyle='--')  # Dashed diagonal line for random classifier
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend()
    plt.show()

from sklearn.model_selection import cross_val_score

# Perform 5-fold cross-validation on the training data
cv_scores = cross_val_score(rf_important, X_train, y_train, cv=5, scoring='accuracy')
print(f"Cross-Validation Accuracy: {cv_scores.mean():.2f} ± {cv_scores.std():.2f}")

from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_important))  # Replace y_pred with predictions from rf_important

rf_weighted = RandomForestClassifier(class_weight='balanced', random_state=42)
rf_weighted.fit(X_train, y_train)
y_pred_weighted = rf_weighted.predict(X_test)

# Evaluate the weighted model
print(classification_report(y_test, y_pred_weighted))

from sklearn.metrics import precision_recall_curve
y_prob_weighted = rf_weighted.predict_proba(X_test)[:, 1]

# Get precision, recall values for different thresholds
precision, recall, thresholds = precision_recall_curve(y_test, y_prob_weighted)

# Plot precision and recall as a function of threshold
plt.plot(thresholds, precision[:-1], label="Precision")
plt.plot(thresholds, recall[:-1], label="Recall")
plt.xlabel("Threshold")
plt.legend()
plt.show()

from sklearn.model_selection import GridSearchCV

# Hyperparameter grid to tune
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10]
}

# Use GridSearchCV to tune hyperparameters
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Best parameters
print(f"Best Parameters: {grid_search.best_params_}")